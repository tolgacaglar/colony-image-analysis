{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Dependencies\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import colony_iomethods as cm\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder for the files\n",
    "exp_name = \"EQ59_Single_Colony_TilesScan.lif\"\n",
    "acq_name = \"3dTimeScan_12h_init\"\n",
    "\n",
    "# base_folder = f\"C:/Users/Tolga/Dropbox/GitHub/colony-image-analysis/{exp_name}/{acq_name}/\"\n",
    "base_folder = f\"D:/Tolga/Colony Images/{exp_name}/{acq_name}/\"\n",
    "\n",
    "# Replace the metadata_path with\n",
    "metadata_path = base_folder + f\"MetaData/{exp_name}_{acq_name}_Properties.xml\"\n",
    "\n",
    "tree = ET.parse(metadata_path)    # xml tree of the current stage position\n",
    "root = tree.getroot()           # root of the xml tree\n",
    "\n",
    "image_xml = root[0]\n",
    "\n",
    "# Video folder\n",
    "video_folder = base_folder + \"Videos_Unmarked/\"\n",
    "# Data folder for analysis:\n",
    "resource_folder = base_folder + \"Resources/\"\n",
    "csv_path = resource_folder + f\"{acq_name}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect information from the xml\n",
    "dim_desc = cm.collectImageDim(image_xml)\n",
    "xsz = dim_desc[\"xsz\"]\n",
    "ysz = dim_desc[\"ysz\"]\n",
    "zsz = dim_desc[\"zsz\"]\n",
    "xvoxel = dim_desc[\"xvoxel\"]\n",
    "yvoxel = dim_desc[\"yvoxel\"]\n",
    "zvoxel = dim_desc[\"zvoxel\"]\n",
    "xunit = dim_desc[\"xunit\"]\n",
    "yunit = dim_desc[\"yunit\"]\n",
    "zunit = dim_desc[\"zunit\"]\n",
    "\n",
    "# Tilescan info\n",
    "tilescan_desc = cm.collectTileScan(image_xml)\n",
    "xixar = tilescan_desc[\"xix_unique_ar\"]\n",
    "yixar = tilescan_desc[\"yix_unique_ar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important!! \n",
    "### Each frame is rescaled from a tilescan of 1024x1024 images, to a final tile_xcnt*1024 x tile_ycnt*1024 tif files.\n",
    "* Collect the tilescan positions and obtain the scalex and scaley\n",
    "\n",
    "* Scale xvoxel and yvoxel accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tilescan_desc = cm.collectTileScan(image_xml)\n",
    "\n",
    "tile_xcnt = tilescan_desc[\"tile_xcnt\"]\n",
    "tile_ycnt = tilescan_desc[\"tile_ycnt\"]\n",
    "\n",
    "scalex = 1.0/tile_xcnt/2\n",
    "scaley = 1.0/tile_ycnt/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## centerx and centery is manually found from the collect_data_movie notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colony center in pixels as manually measured from the videos\n",
    "centerx_video = 233\n",
    "centery_video = 268\n",
    "# Convert the colony center based on the scale and tilescan info\n",
    "centerx = centerx_video/scalex\n",
    "centery = centery_video/scaley\n",
    "\n",
    "# Dimension properties\n",
    "frame_count = int(zsz)     # Number of z-scans, or number of frames from the videos.\n",
    "frame_height = int(ysz*tile_ycnt)\n",
    "frame_width = int(xsz*tile_xcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame_count)\n",
    "print(frame_height)\n",
    "print(frame_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect image sequence from the merged tilescans\n",
    "- Creating an array of image sequences failed due to low memory. It adds up to ~28.4 GiB of type float64\n",
    "- Instead, move through each large tiff file and do the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced images from merged tilescan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_folder = base_folder + \"Merged/\"\n",
    "\n",
    "# Calculate circular average\n",
    "# LxL blocks\n",
    "pxl_threshold = 4\n",
    "cnt_threshold = 5\n",
    "block_length = 48\n",
    "frame_height = tile_ycnt*ysz\n",
    "frame_width = tile_xcnt*xsz\n",
    "reduced_height = int(frame_height/block_length)\n",
    "reduced_width = int(frame_width/block_length)\n",
    "\n",
    "tix = 7\n",
    "zix = 80\n",
    "print(\"\\rCalculating %d/%d...\" % (zix, frame_count), end=\"\")\n",
    "sys.stdout.flush()\n",
    "reduced_avg_img = np.zeros((reduced_height, reduced_width))\n",
    "reduced_cnt_img = np.zeros_like(reduced_avg_img, dtype=np.int)\n",
    "t_str = \"t%d\" % (tix)\n",
    "z_str = \"z%03d\" % (zix)\n",
    "\n",
    "merged_path = merged_folder + f\"{acq_name}_{t_str}_{z_str}.tif\" # _{t_str} is included only when timepoints are added in acquire.\n",
    "frame = cv2.imread(merged_path)\n",
    "frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "for yix in range(len(reduced_avg_img)):\n",
    "    for xix in range(len(reduced_avg_img[yix])):\n",
    "        block_img = frame_gray[yix*block_length:(yix+1)*block_length,\n",
    "                            xix*block_length:(xix+1)*block_length]\n",
    "        reduced_avg_img[yix,xix] = block_img.mean()\n",
    "        reduced_cnt_img[yix,xix] = (block_img > pxl_threshold).sum()\n",
    "    \n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.imshow(reduced_cnt_img > cnt_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central average for the reduced image obtained from full-sized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = ((reduced_cnt_img > cnt_threshold).astype(np.uint8)*255)\n",
    "(reduced_height, reduced_width) = img.shape\n",
    "\n",
    "Rar_pxl = np.arange(1,int(reduced_height/2), dtype=np.int) # radius in pixels\n",
    "central_sum = np.zeros_like(Rar_pxl, dtype=np.double)\n",
    "\n",
    "for Rix in range(len(Rar_pxl)):\n",
    "    radius = Rar_pxl[Rix]\n",
    "\n",
    "    circle_img = np.zeros((reduced_height,reduced_width), np.uint8)\n",
    "    cv2.circle(circle_img,(int(centerx/block_length),int(centery/block_length)), radius,1,thickness=-1)\n",
    "    masked_data = cv2.bitwise_and(img, img, mask=circle_img)\n",
    "\n",
    "    central_sum[Rix] = masked_data.sum()\n",
    "\n",
    "# Calculate the uniform fit (polyfit with degree=0)\n",
    "uniform_fit = np.zeros_like(Rar_pxl[:-1])\n",
    "for ix in range(len(Rar_pxl[:-1])):\n",
    "    edge_ix = Rar_pxl[ix]\n",
    "    uniform_fit[ix] = np.polyfit(Rar_pxl[edge_ix:], central_sum[edge_ix:], 0)\n",
    "    \n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(Rar_pxl, central_sum)\n",
    "\n",
    "edge_pxl = Rar_pxl[np.abs(np.diff(uniform_fit, prepend=200, append=0)) < 100][0]\n",
    "edge_pxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central average throughout all scanning direcions (z and t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_folder = base_folder + \"Merged/\"\n",
    "\n",
    "# Calculate circular average\n",
    "# LxL blocks\n",
    "pxl_threshold = 4\n",
    "cnt_threshold = 5\n",
    "block_length = 48\n",
    "frame_height = tile_ycnt*ysz\n",
    "frame_width = tile_xcnt*xsz\n",
    "reduced_height = int(frame_height/block_length)\n",
    "reduced_width = int(frame_width/block_length)\n",
    "\n",
    "zar = np.arange(0,frame_count)\n",
    "tar = np.arange(0,8)\n",
    "outer_radius_ar = np.zeros((len(tar), len(zar)))\n",
    "for tix in range(len(tar)):\n",
    "    t_str = \"t%d\" % (tix)\n",
    "    for zix in range(len(zar)):\n",
    "        zstrix = zar[zix]\n",
    "        z_str = \"z%03d\" % (zstrix)\n",
    "\n",
    "        print(\"\\rCalculating %d/%d for tix=%d/%d\" % (zstrix, frame_count, tix, len(tar)), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        reduced_avg_img = np.zeros((reduced_height, reduced_width))\n",
    "        reduced_cnt_img = np.zeros_like(reduced_avg_img, dtype=np.int)\n",
    "\n",
    "        merged_path = merged_folder + f\"{acq_name}_{t_str}_{z_str}.tif\" # _{t_str} is included only when timepoints are added in acquire.\n",
    "        frame = cv2.imread(merged_path)\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        for yix in range(len(reduced_avg_img)):\n",
    "            for xix in range(len(reduced_avg_img[yix])):\n",
    "                block_img = frame_gray[yix*block_length:(yix+1)*block_length,\n",
    "                                    xix*block_length:(xix+1)*block_length]\n",
    "                reduced_avg_img[yix,xix] = block_img.mean()\n",
    "                reduced_cnt_img[yix,xix] = (block_img > pxl_threshold).sum()\n",
    "\n",
    "        img = ((reduced_cnt_img > cnt_threshold).astype(np.uint8)*255)\n",
    "        (reduced_height, reduced_width) = img.shape\n",
    "\n",
    "        Rar_pxl = np.arange(1,int(reduced_height/2), dtype=np.int) # radius in pixels\n",
    "        central_sum = np.zeros_like(Rar_pxl, dtype=np.double)\n",
    "\n",
    "        for Rix in range(len(Rar_pxl)):\n",
    "            radius = Rar_pxl[Rix]\n",
    "\n",
    "            circle_img = np.zeros((reduced_height,reduced_width), np.uint8)\n",
    "            cv2.circle(circle_img,(int(centerx/block_length),int(centery/block_length)), radius,1,thickness=-1)\n",
    "            masked_data = cv2.bitwise_and(img, img, mask=circle_img)\n",
    "\n",
    "            central_sum[Rix] = masked_data.sum()\n",
    "\n",
    "        # Calculate the uniform fit (polyfit with degree=0)\n",
    "        uniform_fit = np.zeros_like(Rar_pxl[:-1])\n",
    "        for ix in range(len(Rar_pxl[:-1])):\n",
    "            edge_ix = Rar_pxl[ix]\n",
    "            uniform_fit[ix] = np.polyfit(Rar_pxl[edge_ix:], central_sum[edge_ix:], 0)\n",
    "        # Calculate the uniform fit (polyfit with degree=0)\n",
    "        try:\n",
    "            outer_radius_ar[tix, zix] = Rar_pxl[np.abs(np.diff(uniform_fit, prepend=200, append=0)) < 100][0]\n",
    "        except IndexError:\n",
    "            outer_radius_ar[tix, zix] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(outer_radius_ar[7,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = base_folder + f\"Resources/{acq_name}_Full.csv\"\n",
    "\n",
    "masked_mean = np.zeros((len(Rar_pxl),frame_count), dtype=np.double)\n",
    "# flip_masked_mean = np.zeros_like(masked_mean)\n",
    "# unmasked_mean = np.zeros_like(masked_mean)\n",
    "# masked_mean = masked_data.sum()/circle_img.sum()\n",
    "# flip_masked_mean = flip_masked_data.sum()/(1-circle_img).sum()\n",
    "# unmasked_mean = img.mean()\n",
    "\n",
    "outer_radius_mm = xsz*tile_xcnt/reduced_height*xvoxel*outer_radius_ar/1e3\n",
    "zar_mm = zar*zvoxel\n",
    "df_dict = {\"zar (mm)\": zar_mm}\n",
    "for tix in range(len(tar)):\n",
    "    t_str = \"rad_t%d (mm)\" % (tix)\n",
    "    df_dict[t_str] = outer_radius_mm[tix,:]\n",
    "    \n",
    "df = pd.DataFrame(data = df_dict)\n",
    "df.to_csv(csv_path)\n",
    "\n",
    "df_metadata = pd.DataFrame({\"type\": \"Full-Image\",\n",
    "                            \"pxl_threshold\": pxl_threshold,\n",
    "                            \"cnt_threshold\": cnt_threshold,\n",
    "                            \"centerx\": centerx,\n",
    "                            \"centery\": centery,\n",
    "                            \"block_length\": block_length},\n",
    "                           index = [0])\n",
    "df_metadata.to_csv(base_folder + f\"Resources/{acq_name}_Full_Metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxl_threshold = 4\n",
    "cnt_threshold = 5\n",
    "block_length = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxl_thresholdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "percent = np.array([0, 0.2, 0.5, 0.8, 1])\n",
    "hour = np.array([1, 1, 2, 2, 2, 3, 4])\n",
    "mint = np.array([29, 49, 9, 35, 58, 33, 6])\n",
    "time = hour + mint/60.\n",
    "\n",
    "odlb = np.zeros((len(percent), len(time)))\n",
    "# 0%\n",
    "odlb[0,:] = [0.291, 0.455, 0.37*2, np.nan, np.nan, np.nan, np.nan]\n",
    "odlb[1,:] = [.27, .402, .335*2, np.nan, np.nan, np.nan, np.nan]\n",
    "odlb[2,:] = [0.182, 0.294, 0.391, 0.353*2, np.nan, np.nan, np.nan]\n",
    "odlb[3,:] = [0.136, 0.191, 0.272, 0.369, 0.6, np.nan, np.nan]\n",
    "odlb[4,:] = [0.096, .107, .124, .165, .222, .323, .506]\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "slopes = np.zeros_like(percent)\n",
    "for pix in range(len(percent)):\n",
    "    mask = (np.isnan(odlb[pix,:]) == False)\n",
    "    p = np.polyfit(time[mask], np.log(odlb[pix,mask]), 1)\n",
    "    slopes[pix] = p[0]\n",
    "    \n",
    "    ax.semilogy(time, odlb[pix,:], 'o')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(2)/slopes*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
